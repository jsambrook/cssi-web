<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="John Sambrook" />
  <meta name="dcterms.date" content="2025-09-27" />
  <title>Explaining SpikingBrain: Brain-Inspired Large Models for Long-Context Efficiency</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    div.abstract {
      margin: 2em 2em 2em 2em;
      text-align: left;
      font-size: 85%;
    }
    div.abstract-title {
      font-weight: bold;
      text-align: center;
      padding: 0;
      margin-bottom: 0.5em;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      border: none;
      border-top: 1px solid #1a1a1a;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Explaining SpikingBrain: Brain-Inspired Large Models
for Long-Context Efficiency</h1>
<p class="author">John Sambrook</p>
<p class="date">September 27, 2025</p>
<div class="abstract">
<div class="abstract-title">Abstract</div>
<p>This explainer distills the main ideas and results from the
SpikingBrain technical report <span class="citation"
data-cites="spikingbrain2025">(Authors 2025)</span>. The work proposes
brain-inspired large language models (LLMs) that replace quadratic
self-attention with linear/hybrid attention and introduce a
spiking-activation scheme to enable event-driven, addition-heavy
computation. The reported systems are trained and served on a non-NVIDIA
GPU cluster (MetaX C550), with a focus on long-context efficiency
(hundreds of thousands to millions of tokens) while maintaining
competitive quality. This article summarizes the architecture, training
approach, benchmark results, efficiency claims, and limitations as
presented by the authors, with brief context on how these ideas relate
to the broader LLM landscape.</p>
</div>
</header>
<h1 id="overview">Overview</h1>
<p>The SpikingBrain report introduces two principal models:</p>
<ul>
<li><p><strong>SpikingBrain-7B</strong>: a 7B-parameter model that uses
pure linear attention.</p></li>
<li><p><strong>SpikingBrain-76B-A12B</strong>: a hybrid-linear
Mixture-of-Experts (MoE) model with roughly 12B active parameters per
token and 76B total.</p></li>
</ul>
<p>A key theme is <em>long-context practicality</em>: by avoiding
quadratic self-attention and exploiting sparsity from a spiking
activation scheme, the authors target stable training and fast inference
at extreme sequence lengths (e.g., <span class="math inline">128</span>k
tokens during training, and illustrative scaling up to
multi-million-token inference).</p>
<h1 id="architectural-ideas">Architectural Ideas</h1>
<h4 id="linear-and-hybrid-attention.">Linear and Hybrid Attention.</h4>
<p>The 7B model uses purely linear attention to eliminate the quadratic
cost of standard self-attention. The 76B model mixes linear and
conventional components in a hybrid MoE, using intra-layer parallel
mixing (versus inter-layer mixing for the 7B). Linear attention ideas
are part of a broader trend aiming to reduce attention complexity (see,
e.g., <span class="citation"
data-cites="vaswani2017 performer2021">(Vaswani et al. 2017; Choromanski
et al. 2021)</span>).</p>
<h4 id="conversion-based-training.">Conversion-Based Training.</h4>
<p>Rather than training from scratch, the report describes remapping
(“converting”) attention and feed-forward weights from an existing
Transformer into linear/low-rank and MoE forms. The authors claim this
can recover most quality with <span class="math inline"> &lt; 2%</span>
of the compute of full-from-scratch training <span class="citation"
data-cites="spikingbrain2025">(Authors 2025)</span>.</p>
<h4 id="adaptive-threshold-spiking.">Adaptive-Threshold Spiking.</h4>
<p>The activations are converted into integer spike counts and sparse
spike trains (binary/ternary/bitwise codings). This encourages
event-driven computation dominated by additions instead of
multiplications and yields substantial activation sparsity in
inference <span class="citation" data-cites="spikingbrain2025">(Authors
2025)</span>.</p>
<h1 id="training-and-data">Training and Data</h1>
<p>The 7B and 76B models are continually pretrained on roughly <span
class="math inline">150</span>–<span class="math inline">160</span>B
tokens according to the report, with subsequent supervised fine-tuning
(SFT) for chat variants. Long-context capability is extended to <span
class="math inline">128</span>k tokens during training, and the system
stack incorporates custom operators and parallelism strategies suitable
for the MetaX platform <span class="citation"
data-cites="spikingbrain2025">(Authors 2025)</span>.</p>
<h1 id="quality-results-selected">Quality Results (Selected)</h1>
<p>The pretrain checkpoints are reported to recover most of the base
model’s quality at substantially reduced compute. Selected
representative results from the report include (exact numbers summarized
by the authors):</p>
<ul>
<li><p><strong>7B (pretrain)</strong>: e.g., MMLU <span
class="math inline"> ≈ 65.8</span>, CMMLU <span
class="math inline"> ≈ 71.6</span>, CEval <span
class="math inline"> ≈ 69.8</span>.</p></li>
<li><p><strong>76B-A12B (pretrain)</strong>: e.g., MMLU <span
class="math inline"> ≈ 73.6</span>, CEval <span
class="math inline"> ≈ 78.6</span>.</p></li>
<li><p><strong>Chat models (SFT)</strong>: e.g., 7B MMLU <span
class="math inline"> ≈ 65.6</span>, 76B MMLU <span
class="math inline"> ≈ 73.7</span>, with higher helpfulness/safety
(“HS”) scores reported for the larger chat model.</p></li>
</ul>
<p>While not state-of-the-art, these values are competitive for the
compute invested and are consistent with the report’s emphasis on
efficiency <span class="citation" data-cites="spikingbrain2025">(Authors
2025)</span>.</p>
<h1 id="long-context-efficiency">Long-Context Efficiency</h1>
<p>Under sequence parallelism, the authors report substantial speedups
in time-to-first-token (TTFT) vs. a conventional baseline, including
<span class="math inline"> ∼ 26.5×</span> at <span
class="math inline">1</span>M tokens and extrapolated <span
class="math inline"> &gt; 100×</span> at <span
class="math inline">4</span>M tokens. The 7B model shows near-constant
TTFT (on the order of <span class="math inline">∼</span>1 s) from <span
class="math inline">256</span>k to <span class="math inline">4</span>M
tokens as GPU count scales (8<span class="math inline">→</span>128).
Training throughput per GPU-second also improves in the long-sequence
regime (e.g., <span class="math inline"> ∼ 5.36×</span> at <span
class="math inline">128</span>k) <span class="citation"
data-cites="spikingbrain2025">(Authors 2025)</span>.</p>
<h1 id="energy-and-sparsity">Energy and Sparsity</h1>
<p>The adaptive spiking scheme yields <span
class="math inline"> ∼ 69%</span> activation sparsity during inference;
with low-precision weights, the report estimates large MAC-energy
reductions vs. FP16/INT8, implying significant energy-efficiency gains.
A compressed 1B-parameter SpikingBrain variant shows up to <span
class="math inline"> ∼ 15.4×</span> decoding speedup at <span
class="math inline">256</span>k tokens on a CPU/mobile stack (via
<code>llama.cpp</code>) <span class="citation"
data-cites="spikingbrain2025">(Authors 2025)</span>.</p>
<h1 id="non-nvidia-platform">Non-NVIDIA Platform</h1>
<p>A notable aspect is the emphasis on large-scale training on a
non-NVIDIA platform (MetaX C550). The report highlights weeks-long
stable runs over hundreds of GPUs, custom operator support, and
inference using open tooling paths (e.g., vLLM-like) adapted to
MetaX <span class="citation" data-cites="spikingbrain2025">(Authors
2025)</span>.</p>
<h1 id="limitations-and-caveats">Limitations and Caveats</h1>
<p>The authors note that some comparison baselines are disadvantaged on
Chinese-centric benchmarks (e.g., CMMLU/CEval) due to their training
data. For ultra-long sequences (<span class="math inline">&gt;</span>2M
tokens), some baseline results are extrapolated due to resource
constraints <span class="citation"
data-cites="spikingbrain2025">(Authors 2025)</span>. As with most
conversion-based approaches, quality can lag best-in-class fully trained
models at equal parameter count.</p>
<h1 id="how-this-fits-in-the-llm-landscape">How This Fits in the LLM
Landscape</h1>
<p>SpikingBrain aligns with ongoing efforts to reduce the cost of
attention and to make long-context processing practical. Linear
attention variants and kernel-based approximations <span
class="citation" data-cites="performer2021">(Choromanski et al.
2021)</span> are natural comparators, as are system-level pathways that
optimize serving (e.g., <em>vLLM</em>-style paged KV caching). In this
context, SpikingBrain’s combination of conversion-based training,
spiking-inspired activations, and demonstrated non-NVIDIA scaling
represents an engineering direction worth tracking.</p>
<h1 id="key-takeaways">Key Takeaways</h1>
<ul>
<li><p>Linear/hybrid attention plus spiking activations can deliver
compelling long-context efficiency.</p></li>
<li><p>Conversion-based training may recover most of a base
Transformer’s quality at a fraction of the compute.</p></li>
<li><p>Targeting non-NVIDIA hardware expands the set of viable platforms
for large-scale LLM training and inference.</p></li>
<li><p>Reported gains (TTFT, throughput, energy) are promising but
should be interpreted alongside the stated caveats.</p></li>
</ul>
<div id="refs" class="references csl-bib-body hanging-indent"
data-entry-spacing="0" role="list">
<div id="ref-spikingbrain2025" class="csl-entry" role="listitem">
Authors, SpikingBrain. 2025. <span>“SpikingBrain Technical Report:
Spiking Brain-Inspired Large Models.”</span> <a
href="https://arxiv.org/abs/2509.05276">https://arxiv.org/abs/2509.05276</a>.
</div>
<div id="ref-performer2021" class="csl-entry" role="listitem">
Choromanski, Krzysztof, Valerii Likhosherstov, David Dohan, Xingyou
Song, Andreea Gane, Tamás Sarlós, Peter Hawkins, et al. 2021.
<span>“Rethinking Attention with Performers.”</span> <em>International
Conference on Learning Representations (ICLR)</em>. <a
href="https://arxiv.org/abs/2009.14794">https://arxiv.org/abs/2009.14794</a>.
</div>
<div id="ref-vaswani2017" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.
<span>“Attention Is All You Need.”</span> In <em>Advances in Neural
Information Processing Systems (NeurIPS)</em>. <a
href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>.
</div>
</div>
</body>
</html>
